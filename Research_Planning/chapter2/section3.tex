\section{拟开展研究课题的国内外研究现状}\label{sec:2.3}

过去的十几年中，神经网络因其优秀的数据感知和学习能力被成功地应用在了各类场景中\cite{cybenko1992approximation,chen1995universal}。
近年来，伴随着硬件计算能力的飞速提升，以深度神经网络为代表的学习模型在
围棋博弈\cite{silver2017mastering}、谈判推演\cite{lewis2017deal}和艺术风格迁移\cite{he2016powerful}等诸多领域取得了突破。
相对于深度学习的工程快速应用，其理论解释则一直滞后。
在实际研究中，神经网络的层链结构与激活函数等参数更多的采用多次调试来予以确定，
对于如何选取合适的神经网络结构或参数以取得更优的学习或生成效果至今仍是挑战。
也因此，学界主流观点认为现在深度学习模型普遍是欠解释或不可解释的，
如何建立一种具有可解释性的学习模型亦是NIPS2017的关注焦点。

为探索神经网络的可解释性，一种可尝试的方法是从小规模的神经网络开始，
逐渐增加神经网络的隐藏层单元直到满足预定的终止条件，
以此观测神经网络中的神经元作用\cite{kwok1997objective}。
众所周知的是，迭代寻找最优隐藏层单元数和权重与偏置值所需要的计算复杂度和开销对数据集的大小是极其敏感的。
因此在处理大规模数据集时，这种方法很难得以应用。

面对大规模数据集，随机算法具有其独特的优势\cite{mahoney2011randomized}。
在神经网络计算中，
随机算法及其思想已证明了其在建立快速学习模型和算法的同时能够大幅度的减少计算开销\cite{scardapane2017randomness,cui2016high}。
同时，基于随机学习算法的神经网络其神经元权重或偏置服从给定概率密度函数下的随机分布。
这种神经元权重或偏置分布确定的神经网络相比纯训练优化神经元权重与偏置的神经网络无疑更具有解释性。

随机学习神经网络一般遵循着两个公认且基本的训练范式，
即随机输入神经网络隐藏层单元的权重与偏置值和给定标准来筛选权重。
在这种范式下，Igelnik和Pao提出了一种随机向量函数链接神经网络（RVFL，Random Version of the Functional-Link Net）\cite{pao1992functional}。
这种通过从给定范围均分分布中确定参数的神经网络在连续函数上具有良好的普适逼近效果。
Tyukin和Prokhorov表明RVFL神经网络需要监督机制来更好逼近目标函数\cite{tyukin2009feasibility}。
并且，Tyukin和Prokhorov的实验显示RVFL网络在随机算法参数设置不合适的情况下会有很高概率无法逼近目标函数。
这一现象在随后的研究中被Gorban等人以通过数学推导的方式加以确认\cite{gorban2016approximation}。
Gorban等人认为RVFL神经网络的设计需要看考虑两个至关重要的参数，即隐藏层单元数量与随机参数范围。
其中，隐藏层单元的数量与模型准确率直接相关，因此隐藏层单元数量需要足够的多。
此外，随机参数的选取也直接影响到模型的逼近效果。
在实际应用中，面对不同规模的数据集，
如何确定合适的RVFL网络隐藏层单元数量和随机参数范围成为了RVFL网络需要解决的首要问题。
针对此问题，Li和Wang希望通过在给定随机参数分布函数的条件下递增RVFL隐藏层单元来确定数据驱动式的RVFL网络\cite{li2017insights}。
Li和Wang发现给定随机参数范围和学习模型收敛速度条件的情况下RVFL网络无法保证其普适逼近能力。
因此，研究保证普适逼近能力前提下神经网络根据数据自适应调整神经网络结构和隐藏层神经元的随机参数范围是十分重要和必要的。
