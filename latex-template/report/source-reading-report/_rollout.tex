\section{对话推演}
\label{section:rollout}
\zihao{-4}

小节\ref{subsection:decoding}中所提的Likelihood-based decoding具有一定的缺陷。
在谈判过程时，代理有两类策略，一是接受对方的要求，二是给出自己的要求，即\quotes{Counter Offer}。
接受要求这一行为在Likelihood-based decoding中会具有更高的概率，
因为接受要求比给出要求更易达成\quotes{Deal}。

本节中所提出的Goal-based decoding将允许更多的谈判策略。
例如，在考虑项目价值和分数之后，代理可能会采用欺骗的手段\wordnote{
    在人-人谈判训练集中，人的表述往往是诚实的，能够较直观的反应自己的需求。
}的获取较高价值的项目，从而拿到更高的分数。

\input{__figure_rollout}
与强化学习不同，FAIR在本节利用代理行为概率$p_\theta$计算期望谈判分数，并且这里的计算考虑代理的未来行为，
如图\ref{figure:rollouts}。
FAIR通过两个阶段来实现对话推演。首先，以$U=u_{0..c}$表示整个谈判过程中的表述\wordnote{
    utterances，包括已出现和可能出现的表述。
}，以$x_{0..n-1}$表示当前的对话历史。
通过计算$x_{n+k+1,T}$对应的$p_\theta$来获取$u=x_{n,n+k}$。
再通过计算$u=x_{n,n+k}$对应的$R(u)$来挑选最优的输出选择$\vec{o}$。
因此该方法下的期望价值分数与对话中的表述相关：
\begin{equation}
R(x_{n.. n+k})=\mathbb{E}_{x_{(n+k+1.. T;\vec{o})}\sim p_\theta}[r(\vec{o})p_\theta(\vec{o}|x_{0..T})]
\end{equation}

接下来返回最大期望价值分数对应的表述：
\begin{equation}
u^{*}=\argmax_{u\in U}R(u)
\end{equation}

FAIR的对话推演为5次，预测轮数为10轮。