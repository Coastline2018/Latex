\section{强化学习}
\zihao{-4}
\label{section:reinforcement}

\ref{section:likelihood}中的监督学习强调模仿谈判代理人的对话，
但并没有明确的表现出想要获取最大项目价值的意图。
因为在这一节，FAIR利用\ref{section:likelihood}中的学习结果进行强化学习。
类似的方法在\citet{LiMonroe-20}和\citet{DasKottur-21}中也有体现。

在强化学习阶段，代理$A$通过读取代理$B$的表述来对自己的模型参数进行优化。
虽然另外一个代理$B$可以是人，FAIR先用之前监督学习出的模型代替人进行尝试。
然而却发现，当两个机器代理同时更新自身模型参数时，它们的对话会与人类语言出现偏离\wordnote{
    此处的偏离是指语法的偏离而非词汇的偏离，词嵌入的特性使得decode过程不会出现新词。
}。
因此FAIR对此节的强化学习模型进行了修正。


在该修正后模型中，代理$A$首先读取它的输入项目数量与价值$\vec{g}$，然后产生表述$x_{0.. n}$。
当$x$产生结束表述标记时，接着从代理$B$那里读取它的回应$x_{n+1.. m}$。
这样往复直到有一方代理产生了结束标志词。
此时，两位代理同时输出选择$\vec{o}$，并记录对应的价值总分\wordnote{
    Reward，即此处的价值总分。若两位代理的选择之和不等于各项目总数，则认为谈判失败即\quotes{Disagree}，
    此时两位代理的价值总分均为0。
}（以下简称分数）。为方便区分两位代理，这里用$X^A$表示代理$A$的行为，即产生的词和选择。

在一个完整的对话生成后，FAIR根据谈判结果对代理$A$的模型参数进行更新。
这里用$r^A$表示代理$A$最终分数，$T$表示对话长度。
$\gamma$表示分数的影响因子，该因子距离对话结束越短影响则越强。
$\mu$表示代理的平均谈判分数，由此FAIR定义了代理行为$x_t\in X^A$的期望谈判分数$R$ ：
\begin{equation}
\label{eq:reinforcement}
R(x_t) = \sum_{x_t\in X^{A}}\gamma^{T-t}(r^A(\vec{o}) - \mu)
\end{equation}

FAIR在这里通过计算代理每一步的期望谈判分数来对参数进行优化：
\begin{equation}
L^{RL}_\theta = \mathbb{E}_{x_{t}\sim p_\theta(x_{t}|x_{0..t-1},\vec{g})}[R(x_t)]
\end{equation}

梯度的计算方法如\citet{Williams-22}，为：
\begin{equation}
\nabla_\theta L^{RL}_\theta = \!\!\!\!\!\sum_{x_t\in X^{A}}\!\!\!\mathbb{E}_{x_t}[R(x_t)\nabla_\theta \log(p_\theta(x_{t}|x_{0..t-1},\vec{g}))]
\end{equation}

值得注意的是，个人认为，FAIR在这里将模型的优化问题转变成了在给定公式下搜索最优解问题，
由此使用梯度下降求解最优，完成优化。

